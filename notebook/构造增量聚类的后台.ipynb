{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeitVWHg71UOPwHpdAQark",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LC1332/Speaker-Grouping/blob/main/notebook/%E6%9E%84%E9%80%A0%E5%A2%9E%E9%87%8F%E8%81%9A%E7%B1%BB%E7%9A%84%E5%90%8E%E5%8F%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- [x] 构造parquet数据，一个增量，一个有待标注（抹掉speaker信息）\n",
        "- [x] 编写load函数\n",
        "- [ ] 编写get_current_table\n",
        "- [ ] （大任务） 编写compute_speaker\n",
        "- [ ] 编写label_row\n",
        "- [ ] 增加一个index 再增加一个单纯用speaker做的"
      ],
      "metadata": {
        "id": "x9KwMZIvaTIj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "这里我们有一个VideoData的类，他里面实际上核心的数据是一个表格\n",
        "\n",
        "# 需求\n",
        "\n",
        "### __init__( inference_table, folder [, previous_tables ] )\n",
        "见load\n",
        "\n",
        "### load( inference_table, folder [, previous_tables ] )\n",
        "inference_table 是当前有待标注的parquet， 包含所有的音频、视频、以及对应的特征信息\n",
        "folder, 音频和视频的存储目录\n",
        "previous_tables 当标注连续剧时，可以载入一个list of filename， 每一个是对应的之前标注的parquet\n",
        "\n",
        "### get_current_table( )\n",
        "获得当前 需要显示在标注工具左侧的table\n",
        "- 如果从来没有自动计算过speaker，这个时候会调用compute_speaker() 来进行计算\n",
        "\n",
        "### compute_spearker( )\n",
        "对整体的表格重新计算 自动推荐的speaker标注\n",
        "-  如果有previous_tables， 会优先考虑标注为 之前的speaker，当然对于陌生的新人，也有概率自动聚类为speakerX\n",
        "- 如果没有previous_tables，都会自动聚类为speakerX\n",
        "\n",
        "### label_row( index, speaker ) 方法\n",
        "将index对应的行标记为speaker\n",
        "并且更新index相关的M近邻的自动标注信息\n",
        "返回整个更新后的表格\n",
        "\n",
        "### label_rows( indexes , speakers )\n",
        "将indexes对应的行进行标记\n",
        "并且更新indexes的M近邻的并集 减去 indexes 之后行的自动标注信息\n",
        "返回更新后的整个表格\n",
        "\n",
        "### get_row_default( index )\n",
        "给出当前行的speaker的自动标注进行返回\n",
        "\n",
        "### get_image_fname( index )\n",
        "给出index对应图片的绝对目录\n",
        "\n",
        "### get_audio_fname( index )\n",
        "给出index对应音频的绝对目录"
      ],
      "metadata": {
        "id": "Kh62_uL5coAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iycfb76Wcoks",
        "outputId": "0d8bc3a3-08f6-4471-b71c-f3a38b8d29b4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 实现增广并查集"
      ],
      "metadata": {
        "id": "KFD-S3Nrv9VU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "class UnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.root = list(range(size))\n",
        "        self.rank = [1] * size\n",
        "\n",
        "    def find(self, x):\n",
        "        if self.root[x] != x:\n",
        "            self.root[x] = self.find(self.root[x])  # Path compression\n",
        "        return self.root[x]\n",
        "\n",
        "    def union(self, x, y):\n",
        "        rootX = self.find(x)\n",
        "        rootY = self.find(y)\n",
        "\n",
        "        if rootX != rootY:\n",
        "            if self.rank[rootX] > self.rank[rootY]:\n",
        "                self.root[rootY] = rootX\n",
        "            elif self.rank[rootX] < self.rank[rootY]:\n",
        "                self.root[rootX] = rootY\n",
        "            else:\n",
        "                self.root[rootY] = rootX\n",
        "                self.rank[rootX] += 1\n",
        "\n",
        "    def connected(self, x, y):\n",
        "        return self.find(x) == self.find(y)\n",
        "```\n",
        "\n",
        "我希望在这个基础上实现一个增广的并查集类，因为我现在合并的时候是使用cosine similarity合并的，所以我希望把union函数 增广为 union( x,y, similarity )\n",
        "\n",
        "并且root上会额外记录最后一次合并的similarity，可以使用get_last_similarity( x )  查询x对应的类最后一次合并的similarity\n",
        "\n",
        "另外增加一个set_root_name 的功能， 支持对某个group对应的类进行命名\n",
        "\n",
        "也有get_root_name(x) 这样的功能，查询x对应的root的name\n",
        "所有的名字一开始初始化为default"
      ],
      "metadata": {
        "id": "skeiSGMHwCod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AugmentedUnionFind:\n",
        "    def __init__(self, size):\n",
        "        self.root = list(range(size))\n",
        "        self.rank = [1] * size\n",
        "        self.size = [1] * size  # Each component initially has one element\n",
        "        self.last_similarity = [-1] * size  # Initialize with -1 or any default value indicating no merge yet\n",
        "        self.root_name = ['default'] * size  # Initialize names as 'default'\n",
        "\n",
        "    def find(self, x):\n",
        "        if self.root[x] != x:\n",
        "            self.root[x] = self.find(self.root[x])  # Path compression\n",
        "        return self.root[x]\n",
        "\n",
        "    def union(self, x, y, similarity):\n",
        "        rootX = self.find(x)\n",
        "        rootY = self.find(y)\n",
        "\n",
        "        if rootX != rootY:\n",
        "            if self.rank[rootX] > self.rank[rootY]:\n",
        "                self.root[rootY] = rootX\n",
        "                self.last_similarity[rootX] = similarity  # Store the similarity of the last merge\n",
        "                self.size[rootX] += self.size[rootY]  # Update the size of the new root\n",
        "            elif self.rank[rootX] < self.rank[rootY]:\n",
        "                self.root[rootX] = rootY\n",
        "                self.last_similarity[rootY] = similarity  # Store the similarity of the last merge\n",
        "                self.size[rootY] += self.size[rootX]  # Update the size of the new root\n",
        "            else:\n",
        "                self.root[rootY] = rootX\n",
        "                self.rank[rootX] += 1\n",
        "                self.last_similarity[rootX] = similarity  # Store the similarity of the last merge\n",
        "                self.size[rootX] += self.size[rootY]  # Update the size of the new root\n",
        "\n",
        "        if self.get_root_name( rootX ) == \"default\" and self.get_root_name( rootY ) != \"default\":\n",
        "            self.set_root_name( rootX, self.get_root_name( rootY ) )\n",
        "\n",
        "    def connected(self, x, y):\n",
        "        return self.find(x) == self.find(y)\n",
        "\n",
        "    def get_last_similarity(self, x):\n",
        "        rootX = self.find(x)\n",
        "        return self.last_similarity[rootX]\n",
        "\n",
        "    def set_root_name(self, x, name):\n",
        "        rootX = self.find(x)\n",
        "        self.root_name[rootX] = name\n",
        "\n",
        "    def get_root_name(self, x):\n",
        "        rootX = self.find(x)\n",
        "        return self.root_name[rootX]\n",
        "\n",
        "    def get_size(self, x):\n",
        "        rootX = self.find(x)\n",
        "        return self.size[rootX]\n",
        "\n",
        "# Example usage\n",
        "uf = AugmentedUnionFind(10)\n",
        "uf.union(1, 2, 0.9)\n",
        "uf.union(2, 5, 0.85)\n",
        "uf.union(5, 6, 0.88)\n",
        "uf.set_root_name(1, \"Group1\")\n",
        "\n",
        "print(uf.get_last_similarity(1))  # Output: 0.88\n",
        "print(uf.get_root_name(1))  # Output: \"Group1\"\n",
        "print(uf.get_size(1))  # Output: 4 (since elements 1, 2, 5, 6 are in the same component)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBiuiEpYwDQg",
        "outputId": "e87bd2c9-78fe-445c-c8d4-c40a948a6eec"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.88\n",
            "Group1\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "构造测试数据"
      ],
      "metadata": {
        "id": "wdSdYr58iY-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "previous_tables = [\n",
        "    '/content/drive/MyDrive/Speaker/feature/liangjian_10_feature.parquet',\n",
        "    '/content/drive/MyDrive/Speaker/feature/亮剑12.parquet',\n",
        "    '/content/drive/MyDrive/Speaker/feature/亮剑13.parquet']\n",
        "\n",
        "inference_table = '/content/drive/MyDrive/Speaker/feature/亮剑20.parquet'\n",
        "\n",
        "# 这里我们需要把inference_table copy到/content/ 然后把 \"人物\" 这一列全部变为none (na)，用pandas读取之后清空后保存到/content\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import shutil\n",
        "\n",
        "def remove_names(original_path):\n",
        "    # Extract the filename from the original path\n",
        "    filename = os.path.basename(original_path)\n",
        "    # Construct the new path under /content/\n",
        "    destination_path = os.path.join('/content/', filename)\n",
        "\n",
        "    # Copy the file from the original location to /content/\n",
        "    shutil.copy(original_path, destination_path)\n",
        "\n",
        "    # Read the parquet file using pandas\n",
        "    df = pd.read_parquet(destination_path)\n",
        "\n",
        "    # Replace the '人物' column with None (NA)\n",
        "    df['人物'] = None\n",
        "\n",
        "    # Save the modified DataFrame back to the new path\n",
        "    df.to_parquet(destination_path)\n",
        "\n",
        "    return destination_path\n",
        "\n",
        "ground_truth_table = inference_table\n",
        "inference_table = remove_names('/content/drive/MyDrive/Speaker/feature/亮剑20.parquet')\n",
        "print(f\"Modified file saved to: {inference_table}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "267FzF8ygnHO",
        "outputId": "541346d8-146e-4dd8-bb8a-cc6f00c92148"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modified file saved to: /content/亮剑20.parquet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_name(text):\n",
        "    # Split the text at the first underscore\n",
        "    parts = text.split('_', 1)\n",
        "    # If an underscore was found, return the part before it; otherwise, return the entire text\n",
        "    return parts[0]\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"example_name_with_underscores\"\n",
        "print(clean_name(sample_text))  # Output: 'example'\n",
        "\n",
        "sample_text_no_underscore = \"exampleName\"\n",
        "print(clean_name(sample_text_no_underscore))  # Output: 'exampleName'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kEOyu6Io-R2",
        "outputId": "c5621bda-ba25-4b67-a908-89de124b31f4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example\n",
            "exampleName\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class VideoData:\n",
        "    def __init__(self, inference_table, folder, previous_tables=[]):\n",
        "\n",
        "        self.load_inference(inference_table, folder)\n",
        "        self.load_previous_tables(previous_tables)\n",
        "\n",
        "        self.M = 5\n",
        "        self.minimal_self_group_size = 3\n",
        "        self.name_epsilon = 0.1\n",
        "        self.stop_threshold = 0.45\n",
        "\n",
        "    def load_inference(self, inference_table, folder ):\n",
        "\n",
        "        self.fname_table = inference_table\n",
        "\n",
        "        # Load the inference table assuming it's already processed and path updated\n",
        "        self.table = pd.read_parquet(self.fname_table)\n",
        "\n",
        "        self.folder = folder\n",
        "\n",
        "        print(f\"Inference table loaded from {self.fname_table}\")\n",
        "\n",
        "    def load_previous_tables(self, previous_tables ):\n",
        "        self.previous_table_fnames = previous_tables\n",
        "        self.merge_and_clean( previous_tables )\n",
        "\n",
        "    def merge_and_clean(self, previous_table_fnames):\n",
        "        # Concatenate all tables if there are previous tables\n",
        "        if len(previous_table_fnames) == 0:\n",
        "            self.previous_data = None\n",
        "            return\n",
        "\n",
        "        previous_tables = [pd.read_parquet(fname) for fname in previous_table_fnames]\n",
        "\n",
        "        all_data = pd.concat(previous_tables, ignore_index=True)\n",
        "\n",
        "        # Clean data: remove rows where '人物' is NA or 'audio_feature' length is 0\n",
        "        all_data.dropna(subset=['人物'], inplace=True)\n",
        "        all_data = all_data[all_data['audio_feature'].apply(len) != 0]\n",
        "\n",
        "        # Assign the cleaned data back to self.table\n",
        "        self.previous_data = all_data\n",
        "        print(\"Data merged and cleaned. Rows with empty '人物' or 'audio_feature' removed.\")\n",
        "\n",
        "    def compute_speaker(self):\n",
        "        self.build_edge_map()\n",
        "        self.sort_edges()\n",
        "        self.group_all()\n",
        "        self.append_similarity()\n",
        "        self.compute_knn_result()\n",
        "\n",
        "    def compute_knn_result(self):\n",
        "        self.table['knn_speaker'] = None\n",
        "        self.table['knn_similarity'] = None\n",
        "        for index, edges in self.candidate_edges_on_previous.items():\n",
        "            if edges:  # Ensure there is at least one edge to process\n",
        "                top_match_index, top_match_similarity = edges[0]\n",
        "                speaker_name = self.previous_data.iloc[top_match_index]['人物']\n",
        "                self.table.at[index, 'knn_speaker'] = speaker_name\n",
        "                self.table.at[index, 'knn_similarity'] = 1 - top_match_similarity\n",
        "        print(\"KNN speakers and similarities computed.\")\n",
        "\n",
        "\n",
        "    def build_edge_map(self):\n",
        "        # Extract and normalize audio features from self.table\n",
        "        audio_features_self = np.stack(self.table['audio_feature'])\n",
        "        norms_self = np.linalg.norm(audio_features_self, axis=1, keepdims=True)\n",
        "        normalized_audio_features_self = audio_features_self / norms_self\n",
        "\n",
        "        # Fit and query NearestNeighbors for self.table\n",
        "        knn_self = NearestNeighbors(n_neighbors=self.M + 1, metric='cosine')\n",
        "        knn_self.fit(normalized_audio_features_self)\n",
        "        distances_self, indices_self = knn_self.kneighbors(normalized_audio_features_self)\n",
        "\n",
        "        # Store the indices and distances for self, ignoring the point itself in the indices\n",
        "        self.candidate_edges_on_self = {i: list(zip(indices_self[i][1:self.M+1], distances_self[i][1:self.M+1])) for i in range(len(self.table))}\n",
        "\n",
        "        if self.previous_data is not None:\n",
        "            # Extract and normalize audio features from self.previous_data\n",
        "            audio_features_previous = np.stack(self.previous_data['audio_feature'])\n",
        "            norms_previous = np.linalg.norm(audio_features_previous, axis=1, keepdims=True)\n",
        "            normalized_audio_features_previous = audio_features_previous / norms_previous\n",
        "\n",
        "            # Fit and query NearestNeighbors for self.previous_data\n",
        "            knn_previous = NearestNeighbors(n_neighbors=self.M, metric='cosine')\n",
        "            knn_previous.fit(normalized_audio_features_previous)\n",
        "            distances_previous, indices_previous = knn_previous.kneighbors(normalized_audio_features_self)\n",
        "\n",
        "            # Store the indices and distances for previous data\n",
        "            self.candidate_edges_on_previous = {i: list(zip(indices_previous[i], distances_previous[i])) for i in range(len(self.table))}\n",
        "\n",
        "        print(\"Edge maps built for self and previous data with distances included.\")\n",
        "\n",
        "    def append_similarity(self):\n",
        "        # Extract the list of unique speakers from previous_data\n",
        "        if self.previous_data is not None:\n",
        "            speakers_set = set(self.previous_data['人物'])\n",
        "        else:\n",
        "            speakers_set = set()\n",
        "\n",
        "        # Iterate over each row in self.table to append similarity to the 'estimated_speaker' column\n",
        "        for idx, row in self.table.iterrows():\n",
        "            estimated_speaker = row['estimated_speaker']\n",
        "            if estimated_speaker not in speakers_set:\n",
        "                # If the estimated speaker is not in the speakers list, continue to next row\n",
        "                continue\n",
        "\n",
        "            # Find the first match from self.candidate_edges_on_previous that corresponds to this speaker\n",
        "            for edge in self.candidate_edges_on_previous.get(idx, []):\n",
        "                target_index, cosine_distance = edge\n",
        "                target_speaker = self.previous_data.iloc[target_index]['人物']\n",
        "                if target_speaker == estimated_speaker:\n",
        "                    # Calculate the cosine similarity and format it\n",
        "                    cosine_similarity = 1 - cosine_distance\n",
        "                    formatted_similarity = \"{:.2f}\".format(cosine_similarity)\n",
        "                    self.table.at[idx, 'estimated_speaker'] = f\"{estimated_speaker}_{formatted_similarity}\"\n",
        "                    break\n",
        "            else:\n",
        "                # If no matching speaker was found, set similarity to 0\n",
        "                self.table.at[idx, 'estimated_speaker'] = f\"{estimated_speaker}_0.00\"\n",
        "\n",
        "        print(\"Similarity values appended to the estimated speakers.\")\n",
        "\n",
        "\n",
        "    def sort_edges(self):\n",
        "        # Aggregate and sort edges\n",
        "        all_edges = []\n",
        "\n",
        "        # Process edges on self\n",
        "        for source_index, edges in self.candidate_edges_on_self.items():\n",
        "            all_edges.extend((source_index, \"self\", target_index, 1 - dist) for target_index, dist in edges)\n",
        "\n",
        "        # Process edges on previous if available\n",
        "        if self.previous_data is not None:\n",
        "            for source_index, edges in self.candidate_edges_on_previous.items():\n",
        "                all_edges.extend((source_index, \"previous\", target_index, 1 - dist) for target_index, dist in edges)\n",
        "\n",
        "        # Sort edges by similarity (1 - distance) in descending order\n",
        "        self.sorted_edges = sorted(all_edges, key=lambda x: x[3], reverse=True)\n",
        "        print(\"Edges sorted by descending similarity.\")\n",
        "\n",
        "    def group_all(self):\n",
        "        # Initialize the union-find structure for the size of the table\n",
        "        uf = AugmentedUnionFind(len(self.table))\n",
        "        self.table['estimated_speaker'] = None\n",
        "\n",
        "        # If there are marked '人物' (speaker) values, copy them to 'estimated_speaker'\n",
        "        if '人物' in self.table.columns:\n",
        "            self.table['estimated_speaker'] = self.table['人物']\n",
        "\n",
        "        # Initialize undeal_count and visited list\n",
        "        undeal_count = len(self.table) - self.table['estimated_speaker'].count()\n",
        "        visited = [False] * len(self.table)\n",
        "\n",
        "        count = 0\n",
        "        count_empty_merge = 0\n",
        "\n",
        "        # Process each edge in the sorted list\n",
        "        for source_index, table_type, target_index, similarity in self.sorted_edges:\n",
        "            if visited[source_index]:\n",
        "                continue\n",
        "\n",
        "            if similarity < self.stop_threshold:\n",
        "                break\n",
        "\n",
        "            # Check if the node has been dealt with\n",
        "            root_name = uf.get_root_name(source_index)\n",
        "\n",
        "\n",
        "            if table_type == \"self\":\n",
        "                target_name = uf.get_root_name(target_index)\n",
        "                if root_name == \"default\" or target_name == \"default\" or root_name == target_name:\n",
        "                    # 这个时候要检查target是不是有root_name\n",
        "                    uf.union(source_index, target_index, similarity)\n",
        "\n",
        "                if root_name == \"default\" and target_name == \"default\":\n",
        "                    count_empty_merge += 1\n",
        "\n",
        "            elif table_type == \"previous\":\n",
        "                target_name = self.previous_data.iloc[target_index]['人物']\n",
        "                last_similarity = uf.get_last_similarity(source_index)\n",
        "\n",
        "                if similarity >= last_similarity - self.name_epsilon or uf.get_size(source_index) < self.minimal_self_group_size:\n",
        "                    if root_name == \"default\":\n",
        "                        uf.set_root_name(source_index, target_name)\n",
        "\n",
        "            root_name = uf.get_root_name(source_index)\n",
        "\n",
        "            if root_name != 'default':\n",
        "                self.table.at[source_index, 'estimated_speaker'] = root_name\n",
        "\n",
        "            # continue\n",
        "\n",
        "            new_size = uf.get_size(source_index)\n",
        "\n",
        "            if not visited[source_index] and (root_name != \"default\" or  new_size >= self.minimal_self_group_size):\n",
        "                visited[source_index] = True\n",
        "                undeal_count -= 1\n",
        "\n",
        "            count += 1\n",
        "\n",
        "            if undeal_count == 0:\n",
        "                break\n",
        "\n",
        "        print(\"无名合并次数\", count_empty_merge)\n",
        "        print(\"合并次数\",count)\n",
        "\n",
        "        # Final pass to ensure all entries have correct labels\n",
        "        for i in range(len(self.table)):\n",
        "            root_name = uf.get_root_name(i)\n",
        "            if root_name != 'default':\n",
        "                self.table.at[i, 'estimated_speaker'] = root_name\n",
        "            else:\n",
        "                root_id = uf.find(i)\n",
        "                self.table.at[i, 'estimated_speaker'] = str(root_id)\n",
        "\n",
        "        print(\"All groups computed and speakers estimated.\")\n",
        "\n",
        "    # Add this method in the class where you define the rest of your methods.\n",
        "\n",
        "    def get_current_table(self):\n",
        "        # Check if 'estimated_speaker' column exists, compute if not\n",
        "        if 'estimated_speaker' not in self.table.columns:\n",
        "            self.compute_speaker()\n",
        "\n",
        "        # Check and select the necessary columns to be returned\n",
        "        required_columns = ['knn_speaker', 'knn_similarity', 'estimated_speaker', '人物', '人物台词', '开始时间']\n",
        "        # Ensure that all required columns exist in the DataFrame\n",
        "        available_columns = [col for col in required_columns if col in self.table.columns]\n",
        "\n",
        "        # Assemble 'knn_result' from 'knn_speaker' and 'knn_similarity'\n",
        "        self.table['knn_result'] = self.table['knn_speaker'].astype(str) + \"_\" + self.table['knn_similarity'].apply(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "        # Reset the index to add it as a column in the DataFrame\n",
        "        result_table = self.table[available_columns + ['knn_result']].reset_index()\n",
        "        # Rename 'index' column to something more descriptive if desired, e.g., 'Row Index'\n",
        "        result_table.rename(columns={'index': 'Row Index'}, inplace=True)\n",
        "\n",
        "        # Reset the index to add it as a column in the DataFrame\n",
        "        result_table = self.table[['knn_result'] + available_columns].reset_index()\n",
        "        # Drop 'knn_speaker' and 'knn_similarity' from the result table\n",
        "        result_table.drop(columns=['knn_speaker', 'knn_similarity'], inplace=True)\n",
        "\n",
        "        # Rename 'index' column to something more descriptive if desired, e.g., 'Row Index'\n",
        "        result_table.rename(columns={'index': 'Row Index'}, inplace=True)\n",
        "\n",
        "        # Move 'Row Index' to be the last column if you want it at the end instead of the beginning\n",
        "        cols = result_table.columns.tolist()  # Convert columns to list\n",
        "        # Ensure 'knn_result' is the first column\n",
        "        cols = [cols[1]] + cols[2:] + [cols[0]]  # Skip 'Row Index' and append it at the end\n",
        "        result_table = result_table[cols]\n",
        "\n",
        "        # Return the DataFrame with the modified columns\n",
        "        return result_table\n",
        "\n",
        "    def label_row(self, index, speaker, if_return = True):\n",
        "        # Update the specified row's speaker\n",
        "        self.table.at[index, '人物'] = speaker\n",
        "\n",
        "        # Extract the audio feature for the specified index row\n",
        "        audio_feature_index = np.array(self.table.at[index, 'audio_feature'])\n",
        "\n",
        "        # Normalize the audio feature vector for cosine similarity calculation\n",
        "        norm_index = np.linalg.norm(audio_feature_index)\n",
        "        if norm_index > 0:\n",
        "            audio_feature_index /= norm_index\n",
        "\n",
        "        # Iterate over all rows except the index row itself\n",
        "        for idx, row in self.table.iterrows():\n",
        "            if idx == index:\n",
        "                continue\n",
        "\n",
        "            # Extract and normalize the audio feature of the current row\n",
        "            audio_feature_current = np.array(row['audio_feature'])\n",
        "            norm_current = np.linalg.norm(audio_feature_current)\n",
        "            if norm_current > 0:\n",
        "                audio_feature_current /= norm_current\n",
        "\n",
        "            # Calculate cosine similarity\n",
        "            similarity = np.dot(audio_feature_index, audio_feature_current)\n",
        "\n",
        "            # Check if this similarity is greater than the existing knn_similarity\n",
        "            if similarity > row['knn_similarity']:\n",
        "                self.table.at[idx, 'knn_speaker'] = speaker\n",
        "                self.table.at[idx, 'knn_similarity'] = similarity\n",
        "\n",
        "                # Assemble 'knn_result' from 'knn_speaker' and 'knn_similarity'\n",
        "                self.table.at[idx, 'estimated_speaker'] = speaker + \"_\" + f\"%.2f\" % similarity\n",
        "\n",
        "        if if_return:\n",
        "            # Return the updated table\n",
        "            return self.get_current_table()\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def label_rows(self, indexes, speakers ):\n",
        "        # 这里有点懒了直接批量调用\n",
        "        for index, speaker in zip(indexes, speakers):\n",
        "            self.label_row(index, speaker, False)\n",
        "\n",
        "        return self.get_current_table()\n",
        "\n",
        "\n",
        "# Usage example\n",
        "folder_path = None\n",
        "inference_table_path = '/content/亮剑20.parquet'\n",
        "previous_tables_paths = [\n",
        "    '/content/drive/MyDrive/Speaker/feature/亮剑12.parquet',\n",
        "    '/content/drive/MyDrive/Speaker/feature/亮剑13.parquet'\n",
        "]\n",
        "\n",
        "video_data = VideoData(inference_table_path, folder_path, previous_tables_paths)\n",
        "print(video_data.previous_data.info())\n",
        "\n",
        "video_data.compute_speaker()\n",
        "\n",
        "print(video_data.candidate_edges_on_self[5])\n",
        "print(video_data.candidate_edges_on_previous[5])\n",
        "\n",
        "print(video_data.sorted_edges[:15])\n",
        "\n",
        "\n",
        "vis_table = video_data.get_current_table()\n",
        "print(vis_table)\n",
        "\n",
        "# Specify the path where you want to save the CSV file\n",
        "output_path = '/content/vis_table.csv'\n",
        "\n",
        "vis_table = video_data.label_row(0,\"赵刚\")\n",
        "vis_table = video_data.label_row(1,\"赵刚\")\n",
        "vis_table = video_data.label_row(2,\"赵刚\")\n",
        "vis_table = video_data.label_row(3,\"赵刚\")\n",
        "\n",
        "vis_table = video_data.label_row(139,\"罗主任\")\n",
        "vis_table = video_data.label_row(146,\"雨田\")\n",
        "\n",
        "# Save the DataFrame to CSV\n",
        "vis_table.to_csv(output_path, index=False)  # Set index=False to not include row indices in the CSV file\n",
        "\n",
        "print(f\"Table saved to {output_path}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrMOBPo8izuN",
        "outputId": "c2a9cfaf-f98e-4a97-aabe-3e8fbf228ed6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference table loaded from /content/亮剑20.parquet\n",
            "Data merged and cleaned. Rows with empty '人物' or 'audio_feature' removed.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1422 entries, 0 to 1859\n",
            "Data columns (total 8 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   人物               1422 non-null   object\n",
            " 1   人物台词             1422 non-null   object\n",
            " 2   开始时间             1422 non-null   object\n",
            " 3   结束时间             1422 non-null   object\n",
            " 4   audio_file       1422 non-null   object\n",
            " 5   screenshot_file  1422 non-null   object\n",
            " 6   visual_feature   1422 non-null   object\n",
            " 7   audio_feature    1422 non-null   object\n",
            "dtypes: object(8)\n",
            "memory usage: 100.0+ KB\n",
            "None\n",
            "Edge maps built for self and previous data with distances included.\n",
            "Edges sorted by descending similarity.\n",
            "无名合并次数 878\n",
            "合并次数 1017\n",
            "All groups computed and speakers estimated.\n",
            "Similarity values appended to the estimated speakers.\n",
            "KNN speakers and similarities computed.\n",
            "[(6, 0.45110273), (62, 0.45453465), (20, 0.50835407), (699, 0.5186975), (2, 0.5199853)]\n",
            "[(1021, 0.52180123), (1348, 0.54249716), (1344, 0.5499964), (1396, 0.5577303), (1350, 0.5670636)]\n",
            "[(98, 'self', 98, 1.0), (99, 'self', 98, 1.0), (234, 'self', 235, 0.9779855012893677), (235, 'self', 234, 0.9779855012893677), (691, 'self', 699, 0.7984402179718018), (699, 'self', 691, 0.7984402179718018), (106, 'self', 110, 0.790738582611084), (110, 'self', 106, 0.790738582611084), (766, 'self', 772, 0.7696529626846313), (772, 'self', 766, 0.7696529626846313), (316, 'self', 766, 0.7656293511390686), (766, 'self', 316, 0.7656293511390686), (246, 'self', 251, 0.7641158103942871), (251, 'self', 246, 0.7641158103942871), (670, 'self', 699, 0.7575057744979858)]\n",
            "    knn_result estimated_speaker    人物      人物台词          开始时间  Row Index\n",
            "0    刑副团长_0.48                27  None     咱谁也不怕  00:01:34.620          0\n",
            "1      赵刚_0.60                68  None     小鬼子不怕  00:01:35.960          1\n",
            "2      赵刚_0.50           山本_0.00  None   阎王爷咱也不怕  00:01:39.400          2\n",
            "3     楚云飞_0.53          楚云飞_0.53  None        老李  00:01:41.600          3\n",
            "4      赵刚_0.54                68  None  你只是受了点小伤  00:01:42.760          4\n",
            "..         ...               ...   ...       ...           ...        ...\n",
            "781   李云龙_0.57          李云龙_0.57  None     把酒拿出来  00:43:22.100        781\n",
            "782   张大彪_0.52          张大彪_0.52  None       我交工  00:43:22.780        782\n",
            "783   李云龙_0.59               738  None   我绝不掖着藏着  00:43:23.880        783\n",
            "784    护士_0.33               302  None    那不是酒精吗  00:43:25.400        784\n",
            "785    护士_0.39               302  None      又不是酒  00:43:26.860        785\n",
            "\n",
            "[786 rows x 6 columns]\n",
            "Table saved to /content/vis_table.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "我把边表（这里包括了待标注剧集自己到自己 和  自己到之前有标注的spearker）排序后 依次合并\n",
        "\n",
        "- 如果合并两个没有标注的数据，则合并\n",
        "- 如果合并一个有名字的group，一个没有名字的group\n",
        "    - 如果没有名字的group大于等于3个数据 则不合并，保留新group\n",
        "    - 如果没有名字的group小于3个数据，则合并\n",
        "- 循环直到所有数据都加入至少3个数据的group或者被标记上名字"
      ],
      "metadata": {
        "id": "texBshCytgAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(video_data.inverse_edge_map[5:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOvG1SkFk_Cq",
        "outputId": "20369b43-c110-477e-fdca-bf0a3c7073cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[6, 62], [2, 5], [14, 40], [4, 10, 12, 15, 24, 43], [47, 61, 72]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x9OGyJT5ofno"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}